# SafeBEV: Progressive BEV Perception for Autonomous Driving

## Introduction

This repo is associated with the survey paper "[Progressive BEV Perception for Safe Autonomous Driving: From Single-Modality to Multi-Agent Collaborative Perception](https://ieeexplore.ieee.org/document/10321736)", which provides an up-to-date literature survey for BEV perception and an open-source BEV toolbox based on PyTorch. We also introduce the BEV algorithm family, including follow-up work on BEV percepton such as [VCD](https://arxiv.org/abs/2310.15670), [GAPretrain](https://arxiv.org/abs/2304.03105), and [FocalDistiller](https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.html). We hope this repo can not only be a good starting point for new beginners but also help current researchers in the BEV perception community.
<!-- In the literature survey, it includes different modalities (camera, lidar and fusion) and tasks (detection and segmentation). -->
This repository complements the survey **"Progressive BEV Perception for Safe Autonomous Driving"**, which categorizes BEV perception into:
1. SafeBEV 1.0: Single-Modality Vehicle-Side Perception
2. SafeBEV 2.0: Multi-Modality Vehicle-Side Perception
3. SafeBEV 3.0: Multi-Agent Collaborative Perception

üîç This repository includes:
- Summaries of key methods (camera, LiDAR, radar, fusion)
- Datasets for BEV perception (onboard, V2X)
- Open-source implementations
- Challenges & future research directions

üìö [Read the Full Survey Paper (PDF)](./docs/overview.md)

üìå Keywords: BEV, LiDAR, Camera, Radar, V2X, V2I, V2V, Autonomous Driving, Multi-Modal Perception